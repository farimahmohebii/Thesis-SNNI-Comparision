# Thesis-SNNI-Comparison

This repository contains the code for the SNNI comparison framework, which includes different SNNI approaches.
## How to Run Experiments

To run the SNNI comparison framework:


./run.sh

## Please note that due to the large size of input files for Porthos and SCI, it was impossible to upload them on GitHub. They can be generated using EzPC/Athos.

## What the `run.sh` Script Does

### Dependency Check and Installation

The script checks for required dependencies (`openssl`, `g++`, `cmake`, `git`, `make`, and `OpenMP`) and installs them if they are not already installed. This ensures that all necessary tools and libraries are present to run the benchmarks.

### Building Projects

If the Cheetah project is not built, the script uses specific build scripts (`build-deps.sh` and `build.sh`) to compile and set up the required binaries and dependencies for the Cheetah framework.

### Benchmark Management

The script provides functionalities to add new benchmarks, allowing users to expand their testing with different models and datasets. Users can run benchmarks both locally and remotely and even compare results to analyze performance.


### Generating and Saving Metrics

After running a benchmark, the script automatically processes the logs generated during execution and converts them into CSV files. These files contain detailed metrics that can be used for further analysis and visualization.

## Experiment Setup

The experiments were tested on the following setup:

- **vCPUs**: 8 vCPUs (4 physical cores)
- **Memory (RAM)**: 52 GB
- **Operating System**: Ubuntu 22.04.1 LTS
- **Kernel Version**: 6.5.0-1024-gcp
- **System Architecture**: x86_64 (64-bit)
- **Environment**: VM (Virtual Machine)

### Important Note

If you want to add a new benchmark, you will need to modify the `preprocess_image.py` script based on your model input type. Ensure the preprocessing aligns correctly with the specific requirements of your model to avoid any compatibility issues.

## Adding a New Benchmark

To add a new benchmark:

1. **Update the `preprocess_image.py` script** according to the model's input requirements.
   - This script handles the preprocessing of input images to the format required by the models. Modify it as per the input type (e.g., different image sizes, normalization methods, etc.).

2. **Ensure the new benchmark follows the structure and conventions of existing benchmarks to maintain consistency.**
   - This includes directory structures, naming conventions, and configuration file formats.

3. **Use the `add_new_benchmark` function in the `run.sh` script** to guide you through the process of setting up a new benchmark.
   - The script will prompt for necessary information such as benchmark type (SCI or Porthos), model type (ONNX or TensorFlow), model URL, input image URL, and output tensor names.

## Running Benchmarks

- **Locally**: The script supports running the benchmark locally on your machine, utilizing available resources.
- **Remotely**: Users can choose to run the benchmark as a server or client, specifying the port and IP address to connect to a remote server.



## Comparing Benchmark Outputs

The `run.sh` script includes functionality to compare the outputs of different benchmarks:

- It prompts the user to select the CSV files generated from the benchmark runs.
- Uses a Python script (`compare_csv.py`) to compare these CSV files and produce a summary or detailed comparison of the results.

## Directory Structure

Here is an overview of the main directories and files in this repository:

- `EzPC/`: Contains the EzPC framework and relevant scripts used for secure computations.
- `OpenCheetah/`: Contains the OpenCheetah framework and its dependencies required for the experiments.
- `logfiles/`: Stores log files generated from running experiments. These logs contain raw output data and metrics.
- `logfilespm/`: Another directory for log files (used by specific scripts for more detailed log storage).
- `compare_csv.py`: Python script to compare CSV outputs from different benchmark runs.
- `preprocess_image.py`: Python script to preprocess input images for models. This needs to be modified if a new benchmark with a different input format is added.
- `process_logs_client.py`: Script to process logs generated by client-side execution of benchmarks.
- `process_logs_server.py`: Script to process logs generated by server-side execution of benchmarks.
- `run.sh`: Main script to run experiments and benchmarks, handle dependencies, and manage benchmark execution.
- `visualize_csv.py`: Script to visualize CSV data for easier analysis and comparison of benchmark results.
